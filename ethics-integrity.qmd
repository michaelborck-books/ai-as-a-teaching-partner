# Ethics, Data Governance & Integrity

## The Conversation You Must Have

If you implement any of the ideas in this book, you will have this conversation—with students, with colleagues, possibly with administrators:

**"Aren't you just teaching students to cheat?"**

This chapter gives you the framework, language, and evidence to respond confidently. More importantly, it helps you position AI integration not as an academic integrity *problem*, but as an academic integrity *opportunity*—a chance to teach professional ethics and responsible technology use.

---

## Reframing the Question

The traditional framing:
> "How do we prevent students from using AI inappropriately?"

The professional framing:
> "How do we teach students to use AI responsibly in their professional careers?"

**The shift matters.**

The first framing treats AI as a threat to be controlled. The second treats AI literacy as a learning objective to be developed.

As a business educator across any discipline, you're not preparing students for a world without AI. You're preparing them for a world where AI tools will be discipline-specific but ubiquitous. Your graduates will use these tools:

::: {.panel-tabset}

## HR
- Screen resumes and identify candidates
- Draft employment contracts and policies
- Analyse workforce data and predict turnover
- Generate interview questions and assessment criteria
- Summarize complex legislation and case law

## Finance
- Analyse financial statements and identify anomalies
- Generate investment recommendations
- Perform risk assessments and stress testing
- Summarize regulatory requirements and tax implications
- Forecast financial performance

## Supply Chain
- Forecast demand and optimize inventory
- Identify supplier risks and opportunities
- Optimize logistics networks and routes
- Analyse supply chain resilience
- Generate sourcing recommendations

## Marketing
- Analyse customer data and segment audiences
- Generate campaign strategies and content
- Predict customer behaviour and preferences
- Optimize pricing and promotional strategies
- Analyse competitive positioning

## Information Systems
- Generate code and identify bugs
- Design system architectures
- Assess technology risks and security
- Create project plans and estimates
- Analyse requirements and specifications

:::

Your graduates will use these tools. The question is: **Will they use them competently and ethically, or incompetently and recklessly?**

That's what this chapter is about.

---

## The Three-Part Framework for Ethical AI Use

This framework works for talking to students, colleagues, and administrators. It has three components:

### 1. Transparency (Not Prohibition)

**The principle:** Make AI use explicit, expected, and assessable rather than hidden and policed.

**In practice:**
- Tell students exactly when and how they can use AI
- Provide the prompts and tools yourself
- Assess their *use* of AI, not their *avoidance* of AI
- Reward students who identify AI's errors and limitations

**Why this builds integrity:**
When AI use is transparent, students learn to use it openly and responsibly. When it's prohibited, students use it secretly and don't develop critical oversight skills.

### 2. Critical Oversight (Not Blind Reliance)

**The principle:** Teach students that AI is a tool requiring human judgment, not an authority to be trusted.

**In practice:**
- Design assignments where students must critique or override AI outputs
- Require students to identify what AI gets wrong
- Grade students on their ability to improve on AI suggestions
- Show examples of AI failures (bias, errors, oversimplification)

**Why this builds integrity:**
Students learn that using AI thoughtfully is harder than avoiding it. They develop the professional habit of verification and critical thinking.

### 3. Professional Relevance (Not Academic Abstraction)

**The principle:** Connect AI use in coursework to AI use in professional practice.

**In practice:**
- Frame assignments as professional scenarios: "You're the HR manager using AI to draft a policy..."
- Discuss workplace AI ethics: "What happens if your AI resume screening tool discriminates?"
- Teach governance: "Who is accountable when AI-assisted decisions go wrong?"
- Include AI literacy as a stated learning objective in your unit outline

**Why this builds integrity:**
When students see AI use as professional skill development rather than academic shortcut, they engage differently. They're not "cheating the system"—they're practicing for their careers.

---

## Data Governance: The Practical Reality

While your institution may have an approved enterprise LLM with data protections, the reality is that students will use multiple tools. Some will have strong data governance; others won't. This section addresses the data governance considerations you need to discuss with students and build into your assignment design.

### The Data Governance Landscape

Different LLMs handle data differently:

**Enterprise/Approved Tools** (e.g., MS Copilot Enterprise with Curtin license)
- Data is siloed and protected within the enterprise
- Individual user data is isolated
- Training data exclusions in place
- Compliance with institutional requirements
- **Appropriate for:** Course materials, assignments, institutional data

**Consumer/Free Tools** (e.g., ChatGPT free tier, Bing Chat, standard Claude)
- User conversations may be retained for model improvement
- Data could potentially be used for training future models
- Less transparency about data handling
- No institutional protection or agreement
- **Risk:** Course materials, assignment content, student work uploaded here can be incorporated into training data

**The Student Reality**
While you may recommend (or require) students use MS Copilot Enterprise, students will inevitably use other tools:
- More familiar interfaces
- No Curtin login required
- Access on personal devices/accounts
- Peer recommendations
- "Just quickly checking" with ChatGPT

This isn't a failure of your instruction—it's the reality of tool adoption. Your role is to help students make informed choices, not to prevent use of other tools entirely.

### Institutional Context: Curtin University

At Curtin University, staff have the Enterprise license for MS Copilot specifically because:
- **Data Protection**: Your data and your students' work is siloed within Curtin's instance
- **Institutional Compliance**: Meets Curtin's data governance and privacy requirements
- **Professional Standard**: Reflects how enterprise professionals use AI tools
- **Approved Use**: This is the officially sanctioned tool for institutional work

**What This Means in Practice:**
- Course materials and institutional data should be processed through MS Copilot Enterprise
- Student assignments containing course content are safer in MS Copilot Enterprise
- Sensitive institutional information should never go into consumer LLMs
- Teaching students to use the enterprise tool is teaching them to work like professionals

### Data Governance Considerations for Assignment Design

Rather than prohibiting certain tools (impossible to enforce), design assignments that naturally encourage responsible data handling:

#### Strategy 1: Use Generic/Fictional Scenarios

Instead of: "Upload this real case study and ask the AI to analyze it"

Try: "Here's a fictional scenario. Analyze it using the provided AI tool. What would you need to verify before applying this to real data?"

**Benefit:** Students practice with realistic scenarios without uploading sensitive materials.

#### Strategy 2: De-Identification Before Upload

If students need to work with real or realistic data:
- Require them to remove identifying information first
- Create assignment steps: "1) Anonymize data, 2) Upload to AI, 3) Document what you removed"
- Assess their decision-making about what constitutes sensitive information

**Benefit:** Students learn data governance practices they'll use professionally.

#### Strategy 3: Process Documentation Over Output Sharing

Instead of: "Submit your full AI conversation transcript"

Try: "Show the three key prompts you used and explain why you modified your approach between each"

**Benefit:** Students demonstrate thinking without uploading entire conversations with potentially sensitive content.

#### Strategy 4: Explicit Tool Choices in Assignment Design

Be clear about which tool to use:
- "Use MS Copilot Enterprise for this assignment (login with your Curtin credentials)"
- "You may use any AI tool for brainstorming, but final analysis should use MS Copilot Enterprise"
- "If using a non-approved tool, anonymize all case data first"

**Benefit:** Students make informed choices and understand why tool selection matters.

#### Strategy 5: Structured Prompts in Approved Tools

Rather than leaving students to compose prompts in any tool they choose, provide:
- Prepared prompts already in MS Copilot Enterprise
- Shared workspace conversations students can access
- Pre-configured scenarios they interact with, rather than create

**Benefit:** You control what data enters the system while students still develop prompting skills.

### Student-Facing Guidance on Data Governance

Here's language you can adapt for student-facing materials:

```
DATA GOVERNANCE AND AI TOOL SELECTION

We have approved MS Copilot Enterprise for coursework because it protects your data
and the university's data. Here's what this means:

WHAT HAPPENS WITH YOUR DATA:
- MS Copilot Enterprise: Your conversations are siloed within Curtin's secure instance.
  Your data is not used to train other models. Your work is protected.

- Other AI tools (ChatGPT, etc.): Your conversations may be retained and potentially
  used to improve those services. Anything you upload could theoretically be seen
  by the company or used in their training.

WHAT THIS MEANS FOR THIS COURSE:

DO use MS Copilot Enterprise when:
- Working with course materials or case studies
- Analyzing real (or realistic) business scenarios
- Uploading assignment drafts for feedback
- Working with any data you wouldn't want public

DO use other tools when:
- Brainstorming general ideas
- Exploring concepts with simple, generic examples
- Personal learning outside formal assignments

DON'T upload to any AI tool:
- Course materials before they're public
- Student work (yours or classmates') without permission
- Real company data or confidential information
- Anything marked as confidential or proprietary

IF YOU USE OTHER TOOLS:
- Remove identifying information first (anonymize real data)
- Document what you removed and why
- Be prepared to explain your tool choice in class discussion
- Understand that your data may not be protected the same way

PROFESSIONAL PRACTICE:
In your careers, you'll work with different tools in different contexts. This course
teaches you to think about data governance: Where does data go? Who can see it?
What risks exist? These are questions you'll ask professionally, not just in class.
```

### Red Flags: Data Governance Issues

Watch for assignments or discussions where students might be uploading sensitive information inappropriately:

**Red Flag:** Student uploads course materials verbatim into consumer tool
- **Response:** Not acceptable for this assignment. Use MS Copilot Enterprise, or anonymize first.

**Red Flag:** Student shares screenshot of conversation with real client names/data
- **Response:** Opportunity to discuss professional confidentiality and data governance in context.

**Red Flag:** Assignment design that assumes students will upload confidential materials
- **Response:** Redesign to use fictional scenarios or require de-identification first.

**Red Flag:** No mention of data governance in unit outline or assignment instructions
- **Response:** Add explicit guidance about which tools to use and why.

### Institutional Policy Reference

As a Curtin educator, you can reference:
- Curtin's Data Governance Policy
- The terms of the MS Copilot Enterprise license
- Professional standards in your discipline about data handling
- Privacy and confidentiality principles relevant to your field

This grounds data governance in institutional reality, not abstract rules.

---

## Student-Facing Language: Setting Expectations

You need clear, direct communication about AI use. Here's a model you can adapt:

### Example: Unit Outline AI Policy Statement

```
ARTIFICIAL INTELLIGENCE USE IN THIS UNIT

In professional practice across all business disciplines, you will use AI tools
to support decision-making, analysis, and communication. This unit teaches you to
use AI responsibly and critically.

WHEN AI USE IS EXPECTED:
- Assignment 2 (Conversation Simulation / Scenario Analysis): You will interact
  with AI-generated scenarios or personas and demonstrate your professional skills
- Assignment 3 (Self-Assessment): You will use the provided AI critique prompt
  to assess your draft before submission
- [Any other assignments where AI engagement is part of learning objectives]

WHEN AI USE IS PERMITTED:
- Brainstorming ideas and approaches
- Generating practice questions and scenarios for exam preparation
- Checking grammar and clarity in written work
- Exploring concepts you don't fully understand yet
- Researching and understanding professional standards and frameworks

WHEN AI USE IS NOT PERMITTED:
- Final exam (closed book, no technology unless specified)
- Any assignment where instructions explicitly state "no AI tools"
- Any assessment explicitly designed to test recall or your unaided thinking

WHAT YOU MUST DO WHEN USING AI:
- Use it as a tool that supports YOUR thinking, not replaces it
- Critically evaluate AI outputs—don't assume they're correct
- Be able to explain and justify any AI-assisted work in your own words
- Acknowledge AI use where required (e.g., "I used Claude to generate initial
  analysis, which I then critically reviewed and revised based on...")

ACADEMIC INTEGRITY EXPECTATIONS:
Using AI inappropriately (e.g., submitting AI-generated work as your own without
critical engagement) is academic misconduct, just like plagiarism.

If you're ever unsure whether your AI use is appropriate, ask before submitting.
I'm here to help you learn to use these tools well and ethically.
```

### Example: First-Day Class Discussion

**What to say:**

> "Let's talk about AI. Some of you are probably already using ChatGPT or similar tools. Some of you are worried that using AI is cheating. Some of you are wondering if I'm going to try to detect and punish AI use.
>
> Here's my position: **AI tools exist, and you'll use them in your professional careers. My job is to teach you to use them wisely and ethically.**
>
> In this unit, we'll use AI openly in some assignments. You'll learn when AI is helpful, when it's risky, and when human judgment must override AI recommendations. That's a professional skill you'll need.
>
> I'm not interested in playing 'gotcha' with AI detection software. I'm interested in whether you can think critically, justify your decisions, and demonstrate competent professional practice. If you can do that with AI assistance, great. If you use AI to avoid thinking, I'll know—because your work won't demonstrate understanding.
>
> Questions or concerns about this approach?"

**Why this works:**
- Sets a clear, positive tone
- Positions you as a guide, not a cop
- Acknowledges student anxiety
- Makes professional relevance explicit
- Invites dialogue

---

## Designing "Integrity-Resistant" Assignments

Some assignments are easier to misuse with AI than others. Here's how to design assessments that are inherently resistant to misuse:

### Principle 1: Assess Process, Not Just Product

**Vulnerable design:** "Write a 1500-word essay analysing a workplace conflict."
- Student can paste this into AI and submit the output

**Integrity-resistant design:** "Conduct a simulated investigation interview (submit transcript), then audit your own process against procedural fairness criteria."
- Student must engage in real-time conversation (can't be pre-written)
- Assessment focuses on methodology visible in transcript
- Self-audit requires metacognitive engagement

### Principle 2: Require Evidence of Thinking

**Vulnerable design:** "Recommend a solution to this [discipline] problem."
- AI can generate a plausible recommendation

**Integrity-resistant design:** "AI generated three solutions to this problem [provide them]. Critique each option, identify which one is best and why, and explain what the AI got wrong."
- Student must think beyond what AI provided
- Requires critical evaluation, not just generation
- Makes AI outputs the starting point, not the end point

**Examples by discipline:**
- HR: "Critique three AI-generated performance management approaches"
- Finance: "Critique three AI-generated investment recommendations"
- Supply Chain: "Critique three AI-generated supplier selection strategies"
- Marketing: "Critique three AI-generated campaign strategies"

### Principle 3: Make Personal Context Essential

**Vulnerable design:** "Analyse the pros and cons of [generic professional concept]."
- Generic question AI can answer generally

**Integrity-resistant design:** "Based on your earlier [simulation/analysis/project], analyse how [concept] would address the specific situation while meeting [organisational/business requirement]."
- Requires integration of previous personalised work
- Context is unique to each student
- Generic AI response won't fit

**Examples by discipline:**
- HR: "Based on your PIP simulation with Jamie, analyse flexible work approaches"
- Finance: "Based on your company analysis, evaluate investment timing strategies"
- Supply Chain: "Based on your supplier evaluation, analyse relationship strategies"
- Marketing: "Based on your segment analysis, evaluate messaging approaches"

### Principle 4: Assess Revision and Iteration

**Vulnerable design:** Submit final work only
- No visibility into how it was created

**Integrity-resistant design:** Submit first draft, AI feedback received, revised draft, and reflection on changes made
- Process is visible and assessable
- Shows learning trajectory
- Difficult to fake iterative improvement

### Principle 5: Require Justification of Choices

**Vulnerable design:** "Create a recruitment interview guide."
- AI can generate a complete guide

**Integrity-resistant design:** "Create an interview guide. For each question, justify why you chose it, what competency it targets, and what poor response would sound like. Identify two questions the AI generated that you rejected and explain why they were inadequate."
- Requires deep understanding, not just production
- Student must demonstrate judgment beyond AI capability
- Reveals whether they understand what they're submitting

---

## Red Flags for AI Misuse (And How to Address Them)

Even with well-designed assignments, some students will try to misuse AI. Here's how to identify and respond:

### Red Flag 1: Sudden Quality Shift

**What you see:** Student whose previous work was weak suddenly submits sophisticated analysis.

**Response approach:**
- **Don't immediately accuse.** There could be legitimate reasons (they got help from writing center, they finally understood the concept, etc.)
- **Ask questions:** "Your analysis has improved significantly. Can you walk me through your thinking process on this particular section?"
- **Request elaboration:** "This point about organisational justice theory is interesting. Can you explain how you see it applying to this specific scenario?"

**If genuine learning:** They can explain their thinking.
**If inappropriate AI use:** They struggle to explain or elaborate.

### Red Flag 2: Work That Doesn't Match Assignment Context

**What you see:** Student used generic AI response that doesn't fit the specific scenario or constraints you provided.

**Example:** Assignment asked for Australian employment law context, student submitted response referencing US legislation.

**Response approach:**
- **Point out the mismatch:** "I notice you've referenced Title VII of the Civil Rights Act, but this assignment requires Australian context. Can you explain how this applies to our scenario?"
- **Provide opportunity to revise:** "I think you may have used a resource that wasn't contextually appropriate. Please resubmit with correct jurisdictional references."

**Teaching moment:** Use this to discuss the importance of contextual verification when using AI tools professionally.

### Red Flag 3: No Evidence of Process in Process-Based Assessment

**What you see:** Student submitted required components but shows no genuine engagement (e.g., self-audit identifies no mistakes, reflection is superficial).

**Response approach:**
- **Return for revision:** "Your self-audit suggests your performance was perfect. Reflective practice requires identifying areas for growth. Please resubmit with honest self-assessment."
- **Offer guidance:** "Everyone makes mistakes in complex HR conversations. Look specifically at moments where the employee seemed frustrated or defensive—what might you have done differently?"

**Teaching moment:** Explain that honest self-assessment is more valuable than false perfection.

### Red Flag 4: Can't Explain or Defend Work in Person

**What you see:** High-quality written submission, but student can't discuss it in office hours or oral follow-up.

**Response approach:**
- **For high-stakes situations:** Schedule a brief oral examination: "I'd like to discuss your assignment. Can you walk me through your main recommendation and why you chose it?"
- **Frame it as learning:** "I was impressed by your analysis. I'd love to hear more about your thinking process."

**If inappropriate use is confirmed:**
- Follow university academic misconduct procedures
- Use it as a teaching moment about professional accountability

---

## Teaching AI Ethics Through Professional Scenarios

One of the most powerful ways to address integrity is to make it a learning objective. Teach students to identify ethical problems with AI use *through discipline-specific scenarios*.

::: {.panel-tabset}

## HR Exercise: The Flawed AI Termination Memo

**Assignment:**

> "Use AI to draft a termination letter for an employee being dismissed for poor performance after a 60-day PIP.
>
> Then conduct an ethical audit:
> - What did the AI include that could create legal risk?
> - What did the AI omit that's legally required?
> - What tone or language choices are problematic?
> - How would you revise this to ensure procedural fairness?
>
> Your grade is based on how thoroughly you identify problems, not on the quality of AI's original output."

**What students learn:**
- AI can confidently generate legally dangerous content
- They must verify and correct AI outputs
- Professional accountability can't be delegated to AI

## Finance Exercise: The Flawed AI Investment Recommendation

**Assignment:**

> "Use AI to recommend an investment portfolio allocation. Then conduct a critical audit:
> - What assumptions did the AI make about risk tolerance and time horizon?
> - What did the AI miss about current market conditions?
> - What tax or regulatory implications are overlooked?
> - How would you revise this recommendation with your professional judgment?
>
> Your grade is based on how thoroughly you identify problems and limitations, not on the quality of AI's original output."

**What students learn:**
- AI can confidently recommend financially risky strategies
- Assumptions must be verified and challenged
- Professional accountability for recommendations can't be delegated

## Supply Chain Exercise: The Flawed AI Supplier Strategy

**Assignment:**

> "Use AI to recommend a supplier consolidation strategy. Then conduct a critical audit:
> - What supply chain risks did the AI overlook?
> - What supplier relationship and quality considerations are missing?
> - What operational constraints wasn't the AI aware of?
> - How would you revise this strategy with on-the-ground knowledge?
>
> Your grade is based on how thoroughly you identify problems and improvements."

**What students learn:**
- AI can oversimplify complex supply chain decisions
- Operational reality must inform strategy
- Professional judgment about supplier relationships is essential

## Marketing Exercise: The Flawed AI Campaign Strategy

**Assignment:**

> "Use AI to generate a campaign strategy for a target market. Then conduct a critical audit:
> - What customer insights did the AI miss or misinterpret?
> - What competitive or market factors aren't addressed?
> - What cultural or regional sensitivities might cause problems?
> - How would you revise this with real market knowledge?
>
> Your grade is based on how thoroughly you identify problems and improvements."

**What students learn:**
- AI can generate culturally insensitive or market-misaligned strategies
- Customer understanding must verify AI outputs
- Professional judgment about market nuance is irreplaceable

## Information Systems Exercise: The Flawed AI System Design

**Assignment:**

> "Use AI to generate system requirements and architecture for a business problem. Then conduct a critical audit:
> - What technical feasibility concerns exist?
> - What security or compliance risks are overlooked?
> - What integration challenges with existing systems aren't considered?
> - How would you revise this design with technical expertise?
>
> Your grade is based on how thoroughly you identify problems and improvements."

**What students learn:**
- AI can generate technically unrealistic designs
- Feasibility and constraints must be verified
- Professional technical judgment is essential

:::

**Common Learning Outcome Across All Disciplines:**
- AI can confidently generate problematic recommendations
- Critical verification and improvement is necessary
- Professional accountability can't be delegated to AI

### Exercise 2: The AI Bias and Fairness Challenge

**Discipline-specific scenarios:**

::: {.panel-tabset}

## HR: The Biased Resume Screening Tool

> "Your company uses an AI resume screening tool. You notice it consistently ranks candidates from certain universities higher and flags career gaps as negative. Three rejected candidates have complained the process seems unfair.
>
> As the HR manager:
> 1. What are the ethical concerns with this AI tool?
> 2. What's your legal risk?
> 3. Who is accountable for the AI's decisions?
> 4. What would you do to address this situation?"

## Finance: The Biased Credit Risk Model

> "Your company uses an AI credit risk model for loan decisions. You discover it systematically rates applicants from certain zip codes as higher risk, even when other factors are equivalent. Multiple applicants have filed complaints.
>
> As the finance manager:
> 1. What are the ethical and legal concerns?
> 2. What's the regulatory risk?
> 3. Who is accountable for discriminatory decisions?
> 4. What would you do to address this?"

## Supply Chain: The Biased Supplier Rating System

> "Your AI supplier rating system consistently rates suppliers from certain regions lower, even when quality metrics are equivalent. Key suppliers have complained and are considering leaving.
>
> As the supply chain manager:
> 1. What are the fairness and business risks?
> 2. What relationship and reputational damage might occur?
> 3. Who is accountable for biased evaluations?
> 4. How would you address this?"

## Marketing: The Biased Customer Segmentation

> "Your AI customer segmentation tool shows significant demographic bias in targeting. Certain groups are consistently excluded from high-value segment classifications. Customer advocacy groups have raised concerns.
>
> As the marketing manager:
> 1. What are the ethical and business risks?
> 2. What's the reputational impact?
> 3. Who is accountable for discriminatory targeting?
> 4. How would you address this?"

:::

**What students learn (across all disciplines):**
- Algorithmic bias is a real professional issue
- Using AI doesn't eliminate human responsibility
- Professionals must advocate for fair processes even when using technology

### Exercise 3: The Over-Reliance Problem

**Discipline-specific scenarios:**

::: {.panel-tabset}

## HR: The Over-Reliance on Turnover Analysis

> "You used AI to analyse exit interview data and generate turnover reduction recommendations. You presented them to senior management and implemented them. Six months later, turnover has increased.
>
> Reflection questions:
> 1. What might the AI have missed in its analysis?
> 2. What was your professional responsibility before presenting AI recommendations?
> 3. How do you explain this outcome to management?
> 4. What does this teach you about using AI in strategic decisions?"

## Finance: The Over-Reliance on Market Forecasting

> "You used AI to forecast market conditions and recommend investment positions. You presented them to the board and implemented them. Three months later, markets moved contrary to the forecast and positions are significantly underwater.
>
> Reflection questions:
> 1. What factors might the AI have missed?
> 2. What was your professional responsibility in validating the forecast?
> 3. How do you explain this to the board?
> 4. What does this teach you about AI-assisted decisions?"

## Supply Chain: The Over-Reliance on Demand Forecasting

> "You used AI to forecast demand and optimize inventory. You implemented major supplier and inventory changes based on this. Demand changed unexpectedly and you now have significant stockouts.
>
> Reflection questions:
> 1. What assumptions might the AI have made incorrectly?
> 2. What was your responsibility to validate the forecast?
> 3. How do you explain this to operations and customers?
> 4. What does this teach you about AI forecasting?"

:::

**What students learn (across all disciplines):**
- AI analysis isn't inherently correct
- Professional judgment can't be outsourced
- They're accountable for recommendations they present, regardless of AI assistance

---

## Responding to Colleague and Administrator Concerns

You may need to justify your approach to colleagues or administrators who are skeptical about AI integration.

### Concern: "This undermines academic standards"

**Response:**

> "Actually, it raises standards. I'm no longer testing whether students can recall information—I'm testing whether they can apply it in realistic, dynamic scenarios. I'm assessing higher-order thinking: critical evaluation, professional judgment, and ethical reasoning. These are harder to demonstrate than memorization."

### Concern: "How do you know they're learning anything?"

**Response:**

> "I assess their process, not just their final product. I can see their thinking in conversation transcripts, in their critiques of AI outputs, and in their reflective analysis. When students can identify what AI got wrong and explain why, they're demonstrating deep understanding."

### Concern: "This doesn't align with university academic integrity policies"

**Response:**

> "University policies typically prohibit *unacknowledged* or *uncritical* use of external sources. My approach makes AI use acknowledged and requires critical evaluation. Students aren't hiding AI use—they're demonstrating competent use. That's consistent with academic integrity principles, just applied to a new tool."

**Supporting evidence:**
- Many universities are updating policies to allow appropriate AI use
- Professional accreditation bodies are recognizing AI literacy as essential
- Employer expectations include ability to use AI tools responsibly

### Concern: "What if other lecturers don't agree?"

**Response:**

> "That's fine—pedagogical approaches can vary across units. I'm being transparent with students about expectations in *my* unit. If other lecturers prohibit AI use, students can follow those different expectations. Professional practice requires adapting to different contexts anyway—this models that."

---

## The Bigger Picture: AI Literacy as Graduate Capability

Position AI literacy as a graduate capability alongside communication, critical thinking, and ethical practice.

### What AI Literacy Means for Business Graduates (All Disciplines)

**Competent graduates across all disciplines should be able to:**

1. **Identify appropriate use cases**
   - When is AI helpful? (data analysis, initial drafts, generating options, research)
   - When is AI risky? (sensitive decisions, final strategic recommendations, high-stakes judgments)
   - When is human judgment essential? (ethical dilemmas, complex stakeholder situations, judgment calls)

2. **Evaluate AI outputs critically**
   - Does this align with legal/regulatory/professional requirements?
   - Is this ethically sound?
   - What assumptions has the AI made?
   - What context or domain expertise is missing?

3. **Maintain accountability**
   - Understanding that using AI doesn't eliminate professional responsibility
   - Knowing when to verify AI recommendations with subject matter experts
   - Documenting decision-making processes and AI role

4. **Recognize bias and limitations**
   - **HR:** Algorithmic bias in recruitment, performance, compensation
   - **Finance:** Bias in risk models, forecasting overconfidence
   - **Supply Chain:** Oversimplification of complex relationships, geopolitical blindspots
   - **Marketing:** Demographic bias in targeting, cultural insensitivity
   - **IT:** Technical feasibility blindness, security oversights
   - **All disciplines:** Over-generalization of complex situations, missing domain context

**This is professional education, not just academic integrity management.**

---

## A Final Ethical Consideration

Here's a question to leave with:

**Is it ethical to graduate professionals who don't know how to use AI responsibly in their field?**

When your graduates enter the workforce across all business disciplines, they will encounter AI in their work:

::: {.panel-tabset}

## HR
- AI-powered recruitment systems making hiring decisions
- Automated performance monitoring and evaluation tools
- AI chatbots handling employee queries
- Algorithmic workforce management systems

## Finance
- AI-powered investment recommendation systems
- Automated risk assessment and credit scoring
- Algorithmic trading and portfolio management
- AI-generated financial forecasts and analysis

## Supply Chain
- AI demand forecasting and inventory optimization
- Algorithmic supplier selection and evaluation
- Automated logistics optimization
- AI-driven supply chain risk assessment

## Marketing
- AI-powered customer segmentation and targeting
- Algorithmic campaign optimization
- AI-generated content and recommendations
- Automated personalisation at scale

## Information Systems
- AI-assisted code generation and testing
- Automated system design and architecture
- AI-powered security threat detection
- Algorithmic project planning and estimation

:::

If they don't understand how to evaluate these tools critically, advocate for responsible use, and identify when human oversight is essential, **they will cause harm**—not through malice, but through incompetence.

Your responsibility as an educator isn't to protect students from AI. It's to prepare them to be ethical, competent professionals in an AI-augmented world.

Teaching them to use AI transparently, critically, and responsibly in your course isn't lowering standards.

**It's fulfilling your educational duty.**

---

## Your Action Step

Before the Appendices, draft your own AI use statement for your next unit outline. Use the framework from this chapter:

1. **When AI use is expected** (specific assignments)
2. **When AI use is permitted** (general study support)
3. **When AI use is not permitted** (exams, specific constraints)
4. **What students must do** (critical engagement, acknowledgment)
5. **Academic integrity expectations** (consequences of misuse)

Write it in your own voice. Make it clear, direct, and positive.

Then review it against this question: **Would a student reading this understand how to use AI appropriately and why it matters for their professional development?**

---

**Next Section Preview:** The Appendices provide ready-to-use resources: a prompt library you can copy and adapt immediately, a one-hour workshop guide for introducing these ideas to colleagues, and a detailed alignment with Curtin University learning outcomes to show how AI integration supports existing educational goals.
